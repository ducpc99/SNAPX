# scripts/run_local_eval.py
# -------------------------
# Ch·∫°y to√†n b·ªô pipeline S-NAPX Hybrid ·ªü ch·∫ø ƒë·ªô local, h·ªó tr·ª£ g·ªôp nhi·ªÅu file YAML c·∫•u h√¨nh
# v√† d√πng ki·∫øn tr√∫c m·ªõi (HybridSNAP + Explain + Cost-Performance + Guard).

from __future__ import annotations

import argparse
import sys
import time
import threading
from pathlib import Path
from typing import Any, Dict, List

import yaml

# Cho ph√©p import src.* d√π script n·∫±m ngo√†i th∆∞ m·ª•c src
ROOT_DIR = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT_DIR))

# Utils config
from src.utils.config import load_config

# Loader chu·∫©n (S-NAP, A-SAD, T-SAD + split train/val/test)
from src.data.loader import load_task_csv

# Core models
from src.sequence.ensemble import SequenceEnsemble
from src.graph_reasoning.dfg import DFG
from src.graph_reasoning.reasoner import GraphReasoner, GraphReasonerConfig
from src.graph_reasoning.semantics_prior import SemanticsPrior, SemanticsPriorConfig
from src.semantics.prompt_pool import PromptPool, PromptPoolConfig
from src.semantics.inference import SemanticsLLM, RuntimeConfig, GenConfig
from src.hybrid.pipeline import HybridSNAP, HybridConfig

# Guard
from src.guard.config import GuardConfig
from src.guard.core import Guard

# Eval
from src.eval.runner import run_evaluation, EvalSample


# =========================
# GPU monitor (tu·ª≥ ch·ªçn)
# =========================
def _gpu_snapshot() -> str:
    """
    C·ªë g·∫Øng l·∫•y th√¥ng tin GPU/VRAM theo th·ª© t·ª± ∆∞u ti√™n:
    1) pynvml (·ªïn ƒë·ªãnh nh·∫•t)
    2) nvidia-smi (subprocess)
    3) torch.cuda.mem_get_info (cu·ªëi c√πng)
    """
    # 1) pynvml
    try:
        import pynvml  # type: ignore

        pynvml.nvmlInit()
        h = pynvml.nvmlDeviceGetHandleByIndex(0)
        name = pynvml.nvmlDeviceGetName(h).decode("utf-8")
        mem = pynvml.nvmlDeviceGetMemoryInfo(h)
        util = pynvml.nvmlDeviceGetUtilizationRates(h).gpu
        txt = f"{name} | util={util:>3}% | VRAM={mem.used//(1024**2)}/{mem.total//(1024**2)} MiB"
        pynvml.nvmlShutdown()
        return txt
    except Exception:
        pass

    # 2) nvidia-smi
    try:
        import subprocess, shlex

        cmd = "nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits"
        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.DEVNULL, text=True, timeout=2)
        name, util, used, total = [x.strip() for x in out.split(",")]
        util = util if util != "[Unknown Error]" else "?"
        return f"{name} | util={util}% | VRAM={used}/{total} MiB"
    except Exception:
        pass

    # 3) torch (free/total)
    try:
        import torch  # type: ignore

        if torch.cuda.is_available():
            free, total = torch.cuda.mem_get_info()
            name = torch.cuda.get_device_name(0)
            used = (total - free) // (1024**2)
            total_mb = total // (1024**2)
            return f"{name} | VRAM‚âà{used}/{total_mb} MiB"
    except Exception:
        pass

    return "GPU monitor unavailable"


def _gpu_monitor_loop(stop_flag, interval_sec: float = 2.0) -> None:
    while not stop_flag.is_set():
        print(f"[GPU] {_gpu_snapshot()}")
        stop_flag.wait(interval_sec)


# =========================
# Print config summary (schema m·ªõi)
# =========================
def _print_cfg_summary(cfg: Dict[str, Any]) -> None:
    dataset = cfg.get("dataset", {}) or {}
    data = cfg.get("data", {}) or {}
    seq = cfg.get("sequence", {}) or {}
    graph = cfg.get("graph", {}) or {}
    sem = cfg.get("semantics", {}) or {}
    semp = cfg.get("semantics_prior", {}) or {}
    guard = cfg.get("guard", {}) or {}
    hybrid = cfg.get("hybrid", {}) or {}
    evl = cfg.get("eval", {}) or {}
    paths = cfg.get("paths", {}) or {}

    print("‚öôÔ∏è  C·∫•u h√¨nh t√≥m t·∫Øt")
    print("   ‚îú‚îÄ run_name    :", cfg.get("run_name"))
    print("   ‚îú‚îÄ task        :", cfg.get("task", "next_activity"))
    print("   ‚îú‚îÄ dataset     :", dataset.get("path"))
    print("   ‚îú‚îÄ split_file  :", dataset.get("split_file"))
    print("   ‚îú‚îÄ case_id_col :", dataset.get("case_id_col", "model_id"))
    print("   ‚îú‚îÄ prefix_col  :", dataset.get("prefix_col", "prefix"))
    print("   ‚îú‚îÄ label_col   :", dataset.get("label_col", "next"))
    print("   ‚îú‚îÄ prefix_sep  :", dataset.get("prefix_sep", ";"))
    print(
        "   ‚îú‚îÄ data        :",
        f"drop_end={data.get('drop_end')}, invert_labels={data.get('invert_labels')}, "
        f"min_activities={data.get('min_activities')}, load_limit={data.get('load_limit')}",
    )
    print("   ‚îú‚îÄ output_dir  :", paths.get("output_dir", "outputs"))
    print(
        "   ‚îú‚îÄ sequence    :",
        f"alpha={seq.get('alpha',0.5)}, "
        f"markov_order={seq.get('markov_order',2)}, min_count={seq.get('min_count',1)}",
    )
    print(
        "   ‚îú‚îÄ graph       :",
        f"enabled={graph.get('enabled',True)}, "
        f"dfg(min_count={graph.get('dfg',{}).get('min_count',1)}, "
        f"min_ratio={graph.get('dfg',{}).get('min_ratio',0.0)}), "
        f"reasoner={graph.get('reasoner',{})}",
    )
    print(
        "   ‚îú‚îÄ semantics   :",
        f"use_llm={sem.get('use_llm',False)}, model={sem.get('model_name')}, "
        f"4bit={sem.get('load_in_4bit',False)}, max_len={sem.get('max_seq_len',2048)}, mode={sem.get('mode','logprob')}",
    )
    print("   ‚îú‚îÄ sem_prior   :", semp)
    print("   ‚îú‚îÄ guard       :", guard)
    print("   ‚îú‚îÄ hybrid      :", hybrid)
    print(
        "   ‚îî‚îÄ eval        :",
        f"num_samples={evl.get('num_samples')}, ndcg_k={evl.get('ndcg_k',5)}, "
        f"measure_cost={evl.get('measure_cost',False)}",
    )
    print()


# =========================
# Main
# =========================
def main() -> None:
    parser = argparse.ArgumentParser(description="Run Local Evaluation for S-NAPX Hybrid")
    parser.add_argument(
        "--config",
        "-c",
        type=str,
        nargs="+",
        required=True,
        help="ƒê∆∞·ªùng d·∫´n t·ªõi 1 ho·∫∑c nhi·ªÅu file YAML c·∫•u h√¨nh (vd: config/base.yaml config/local_eval.yaml)",
    )
    parser.add_argument(
        "--monitor-gpu",
        action="store_true",
        help="In tr·∫°ng th√°i GPU/VRAM m·ªói v√†i gi√¢y trong khi ch·∫°y",
    )
    parser.add_argument(
        "--gpu-interval",
        type=float,
        default=2.0,
        help="Chu k·ª≥ (gi√¢y) in tr·∫°ng th√°i GPU khi b·∫≠t --monitor-gpu",
    )
    parser.add_argument(
        "--save-merged-cfg",
        action="store_true",
        help="L∆∞u b·∫£n config ƒë√£ merge v√†o th∆∞ m·ª•c output ƒë·ªÉ t√°i l·∫≠p th√≠ nghi·ªám",
    )
    args = parser.parse_args()

    cfg_paths = [str(Path(p)) for p in args.config]

    # 1) Load & merge YAML (d√πng utils/config.py)
    cfg = load_config(cfg_paths)

    # 2) Chu·∫©n b·ªã output dir
    paths_cfg = cfg.get("paths", {}) or {}
    run_name = cfg.get("run_name", "run_local")
    base_output = Path(paths_cfg.get("output_dir", "outputs"))
    out_dir = base_output / run_name
    out_dir.mkdir(parents=True, exist_ok=True)

    print("\nüöÄ  B·∫Øt ƒë·∫ßu ch·∫°y S-NAPX Hybrid (Local Mode)...\n")
    _print_cfg_summary(cfg)

    # (tu·ª≥ ch·ªçn) L∆∞u b·∫£n config ƒë√£ merge ƒë·ªÉ reproducible
    if args.save_merged_cfg:
        merged_cfg_path = out_dir / "run_config_merged.yaml"
        try:
            with open(merged_cfg_path, "w", encoding="utf-8") as f:
                yaml.safe_dump(cfg, f, allow_unicode=True)
            print(f"üìù  ƒê√£ l∆∞u merged config ‚Üí {merged_cfg_path}")
        except Exception as e:
            print(f"[Warn] Kh√¥ng th·ªÉ l∆∞u merged config: {e}")

    # GPU monitor (tu·ª≥ ch·ªçn)
    stop_flag = threading.Event()
    t_monitor = None
    if args.monitor_gpu:
        t_monitor = threading.Thread(
            target=_gpu_monitor_loop,
            args=(stop_flag, args.gpu_interval),
            daemon=True,
        )
        t_monitor.start()

    t0 = time.time()
    try:
        # ==============================
        # 3) Load d·ªØ li·ªáu b·∫±ng load_task_csv (S-NAP + split train/test)
        # ==============================
        dataset_cfg = cfg.get("dataset", {}) or {}
        data_cfg = cfg.get("data", {}) or {}
        eval_cfg = cfg.get("eval", {}) or {}
        task = cfg.get("task", "next_activity")

        dataset_path = dataset_cfg.get("path")
        split_file = dataset_cfg.get("split_file")
        if not dataset_path:
            raise ValueError("dataset.path ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong YAML.")

        load_limit = data_cfg.get("load_limit", None)
        drop_end = data_cfg.get("drop_end", True)
        invert_labels = data_cfg.get("invert_labels", False)
        min_acts = data_cfg.get("min_activities", 2)

        ret = load_task_csv(
            task=task,
            dataset_path=dataset_path,
            limit=load_limit,
            drop_end=drop_end,
            invert_labels=invert_labels,
            min_activities=min_acts,
            split_file=split_file,
            return_splits_if_available=True,
        )

        # N·∫øu c√≥ split_file h·ª£p l·ªá: ret l√† dict {'train','val','test'}
        if isinstance(ret, dict):
            df_train = ret["train"]
            df_test = ret["test"]
            df_val = ret.get("val")
        else:
            df_train = df_test = ret
            df_val = None

        print(f"üì¶  Loaded train={len(df_train)}, test={len(df_test)} t·ª´ {dataset_path}")

        # Eval tr√™n test (c√≥ th·ªÉ gi·ªõi h·∫°n b·∫±ng eval.num_samples)
        num_samples = eval_cfg.get("num_samples")
        if isinstance(num_samples, int) and 0 < num_samples < len(df_test):
            df_eval = df_test.head(num_samples)
            print(f"üîç  Gi·ªõi h·∫°n xu·ªëng {len(df_eval)} m·∫´u test ƒë·ªÉ eval nhanh.")
        else:
            df_eval = df_test

        # T·∫°o EvalSample t·ª´ df_eval
        samples: List[EvalSample] = []
        for _, row in df_eval.iterrows():
            pref = list(row["prefix"])
            y_true = str(row["next"])
            mid = str(row.get("model_id", ""))
            rid = str(row.get("revision_id", "")) if "revision_id" in row else ""
            case_id = f"{mid}_{rid}" if rid != "" else mid
            samples.append(EvalSample(case_id=case_id, prefix=pref, y_true=y_true))

        if not samples:
            raise ValueError("Kh√¥ng c√≥ sample n√†o sau khi t·∫°o EvalSample.")

        print(f"üîç  Eval samples: {len(samples)}")

        # Traces cho Sequence & DFG l·∫•y t·ª´ TRAIN (ƒë√∫ng methodology)
        traces_for_seq: List[List[str]] = []
        for _, row in df_train.iterrows():
            pref = list(row["prefix"])
            nxt = str(row["next"])
            if pref:
                traces_for_seq.append(pref + [nxt])
            else:
                traces_for_seq.append([nxt])

        activities_vocab = sorted({act for tr in traces_for_seq for act in tr})
        print(f"üß©  Num activities in vocab: {len(activities_vocab)}")

        # =======================================
        # 4) SequenceEnsemble + DFG + Reasoner
        # =======================================
        seq_cfg = cfg.get("sequence", {}) or {}
        seq_model = SequenceEnsemble(
            alpha=seq_cfg.get("alpha", 0.5),
            markov_order=seq_cfg.get("markov_order", 2),
            min_count=seq_cfg.get("min_count", 1),
        )
        seq_model.fit(traces_for_seq)
        print("‚úÖ  SequenceEnsemble fitted.")

        graph_cfg = cfg.get("graph", {}) or {}
        dfg_cfg = graph_cfg.get("dfg", {}) or {}
        dfg = DFG.from_traces(
            traces_for_seq,
            min_count=dfg_cfg.get("min_count", 1),
            min_ratio=dfg_cfg.get("min_ratio", 0.0),
        )
        print("‚úÖ  DFG built.")

        reasoner_cfg = graph_cfg.get("reasoner", {}) or {}
        graph_reasoner = GraphReasoner(
            dfg=dfg,
            cfg=GraphReasonerConfig(
                enabled=graph_cfg.get("enabled", True),
                hard_mask=reasoner_cfg.get("hard_mask", False),
                boost_factor=reasoner_cfg.get("boost_factor", 0.5),
                min_prob_keep=reasoner_cfg.get("min_prob_keep", 0.0),
            ),
        )

        # =======================
        # 5) Semantics-Prior
        # =======================
        import json

        semp_cfg = cfg.get("semantics_prior", {}) or {}
        sem_prior_conf = SemanticsPriorConfig(
            enabled=semp_cfg.get("enabled", False),
            mode=semp_cfg.get("mode", "activity"),
            default_weight=semp_cfg.get("default_weight", 1.0),
            lambda_mix=semp_cfg.get("lambda_mix", 0.2),
            floor=semp_cfg.get("floor", 0.7),
            ceil=semp_cfg.get("ceil", 1.3),
        )
        prior_map = {}
        prior_path = semp_cfg.get("prior_map_path")
        if prior_path:
            try:
                with open(prior_path, "r", encoding="utf-8") as f:
                    prior_map = json.load(f)
                print(f"‚úÖ  Loaded semantics prior map t·ª´ {prior_path}")
            except Exception as e:
                print(f"[Warn] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c prior_map t·ª´ {prior_path}: {e}")
        sem_prior = SemanticsPrior(cfg=sem_prior_conf, prior_map=prior_map)

        # =======================
        # 6) PromptPool + LLM
        # =======================
        prompt_pool = PromptPool(PromptPoolConfig())
        sem_cfg = cfg.get("semantics", {}) or {}
        use_llm = sem_cfg.get("use_llm", False)
        sem_llm = None

        if use_llm:
            print("üß†  Kh·ªüi t·∫°o SemanticsLLM (c√≥ th·ªÉ t·ªën th·ªùi gian / VRAM)...")
            runtime_cfg = RuntimeConfig(
                model_name=sem_cfg.get("model_name", "microsoft/Phi-3-mini-4k-instruct"),
                max_seq_len=sem_cfg.get("max_seq_len", 2048),
                load_in_4bit=sem_cfg.get("load_in_4bit", False),
                device=sem_cfg.get("device", "auto"),
            )
            gen_cfg = GenConfig(
                max_new_tokens=sem_cfg.get("gen", {}).get("max_new_tokens", 16),
                temperature=sem_cfg.get("gen", {}).get("temperature", 0.0),
                top_p=sem_cfg.get("gen", {}).get("top_p", 1.0),
                do_sample=sem_cfg.get("gen", {}).get("do_sample", False),
            )
            mode = sem_cfg.get("mode", "logprob")
            try:
                sem_llm = SemanticsLLM(
                    prompt_pool=prompt_pool,
                    runtime_cfg=runtime_cfg,
                    gen_cfg=gen_cfg,
                    mode=mode,
                )
                print("‚úÖ  SemanticsLLM ready tr√™n device:", sem_llm.device)
            except Exception as e:
                print(f"[Warn] Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c SemanticsLLM, fallback sequence-only: {e}")
                sem_llm = None
                use_llm = False
        else:
            print("‚ÑπÔ∏è  semantics.use_llm = False ‚Üí ch·∫°y sequence + graph (kh√¥ng d√πng LLM).")

        # =======================
        # 6b) Guard (footprint / anomaly) ‚Äì FAIL-SOFT
        # =======================
        guard_cfg_raw = cfg.get("guard", {}) or {}
        guard = None
        guard_enabled_flag = guard_cfg_raw.get("enabled", False)

        if guard_enabled_flag:
            gcfg = GuardConfig(
                enabled=True,
                mode=guard_cfg_raw.get("mode", "soft"),
                use_activity_guard=guard_cfg_raw.get("use_activity_guard", True),
                use_trace_guard=guard_cfg_raw.get("use_trace_guard", False),
                penalty_factor=guard_cfg_raw.get("penalty_factor", 0.5),
                min_prefix_len=guard_cfg_raw.get("min_prefix_len", 1),
                activity_dataset=guard_cfg_raw.get("activity_dataset"),
                trace_dataset=guard_cfg_raw.get("trace_dataset"),
            )
            try:
                guard = Guard.from_datasets(gcfg)
                print("‚úÖ  Guard loaded t·ª´ datasets (activity/trace).")
            except Exception as e:
                # Kh√¥ng ƒë·ªÉ Guard l√†m crash to√†n b·ªô pipeline
                print(f"[Warn] Kh√¥ng build ƒë∆∞·ª£c Guard t·ª´ datasets, t·∫°m th·ªùi t·∫Øt Guard: {e}")
                guard = None
                guard_enabled_flag = False
        else:
            print("‚ÑπÔ∏è  guard.enabled = False ‚Üí kh√¥ng d√πng Guard.")

        # =======================
        # 7) HybridSNAP
        # =======================
        hybrid_cfg_raw = cfg.get("hybrid", {}) or {}
        hyb_cfg = HybridConfig(
            top_k=hybrid_cfg_raw.get("top_k", 5),
            enable_graph_reasoner=hybrid_cfg_raw.get("enable_graph_reasoner", True),
            enable_semantics_prior=hybrid_cfg_raw.get(
                "enable_semantics_prior",
                semp_cfg.get("enabled", False),
            ),
            # n·∫øu YAML kh√¥ng ghi r√µ, m·∫∑c ƒë·ªãnh b√°m theo guard_enabled_flag
            enable_guard=hybrid_cfg_raw.get("enable_guard", guard_enabled_flag),
            enable_explanation=hybrid_cfg_raw.get("enable_explanation", False) and bool(sem_llm),
        )

        hybrid = HybridSNAP(
            cfg=hyb_cfg,
            seq_model=seq_model,
            graph_reasoner=graph_reasoner if hyb_cfg.enable_graph_reasoner else None,
            sem_prior=sem_prior if hyb_cfg.enable_semantics_prior else None,
            semantics_llm=sem_llm,
            guard=guard,
            prompt_pool=prompt_pool,
            activities_vocab=activities_vocab,
        )
        print("‚úÖ  HybridSNAP ready.")

        # =======================
        # 8) Evaluation
        # =======================
        ndcg_k = eval_cfg.get("ndcg_k", 5)
        measure_cost = eval_cfg.get("measure_cost", False)

        eval_result = run_evaluation(
            samples=samples,
            model=hybrid,
            activities_vocab=activities_vocab,
            measure_cost=measure_cost,
            ndcg_k=ndcg_k,
        )

    finally:
        if args.monitor_gpu:
            stop_flag.set()
            if t_monitor is not None:
                t_monitor.join(timeout=1)

    elapsed = time.time() - t0

    # =======================
    # 9) In & l∆∞u k·∫øt qu·∫£
    # =======================
    print("\nüìä  K·∫øt qu·∫£ t·ªïng h·ª£p:")
    print("----------------------")
    print(f"Accuracy     : {eval_result.accuracy:.4f}")
    print(f"Macro-F1     : {eval_result.macro_f1:.4f}")
    print(f"MRR          : {eval_result.mrr:.4f}")
    print(f"NDCG@{ndcg_k:<3}: {eval_result.ndcg_at_k:.4f}")

    print("\nüîé  Explain Metrics:")
    for k, v in eval_result.explain_metrics.items():
        print(f"{k:<25}: {v:.4f}")

    if eval_result.latency is not None:
        lat = eval_result.latency
        print("\n‚è±Ô∏è  Latency (ms):")
        print(f"avg_ms        : {lat.avg_ms:.2f}")
        print(f"p50_ms        : {lat.p50_ms:.2f}")
        print(f"p90_ms        : {lat.p90_ms:.2f}")
        print(f"p95_ms        : {lat.p95_ms:.2f}")

    if eval_result.cpr is not None:
        print(f"\n‚öñÔ∏è  CPR (Macro-F1 / avg_sec): {eval_result.cpr:.4f}")

    print("----------------------")
    print(f"‚è±Ô∏è  Th·ªùi gian ch·∫°y: {elapsed/60:.2f} ph√∫t ({elapsed:.1f} gi√¢y)")
    print(f"üìÇ  Output dir   : {out_dir}\n")

    # L∆∞u results.yaml ƒë·ªÉ sau n√†y d√πng cho b√°o c√°o
    results_dict: Dict[str, Any] = {
        "accuracy": eval_result.accuracy,
        "macro_f1": eval_result.macro_f1,
        "mrr": eval_result.mrr,
        "ndcg_at_k": eval_result.ndcg_at_k,
        "explain_metrics": eval_result.explain_metrics,
        "latency_ms": None,
        "cpr": eval_result.cpr,
    }
    if eval_result.latency is not None:
        results_dict["latency_ms"] = {
            "avg_ms": eval_result.latency.avg_ms,
            "p50_ms": eval_result.latency.p50_ms,
            "p90_ms": eval_result.latency.p90_ms,
            "p95_ms": eval_result.latency.p95_ms,
        }

    results_path = out_dir / "results.yaml"
    try:
        with open(results_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(results_dict, f, allow_unicode=True)
        print(f"üìù  ƒê√£ l∆∞u k·∫øt qu·∫£ chi ti·∫øt ‚Üí {results_path}")
    except Exception as e:
        print(f"[Warn] Kh√¥ng th·ªÉ l∆∞u results.yaml: {e}")


if __name__ == "__main__":
    main()
