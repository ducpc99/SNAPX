# scripts/run_local_eval.py
# -------------------------
# Cháº¡y toÃ n bá»™ pipeline S-NAPX Hybrid á»Ÿ cháº¿ Ä‘á»™ local, há»— trá»£ gá»™p nhiá»u file YAML cáº¥u hÃ¬nh
# vÃ  dÃ¹ng kiáº¿n trÃºc má»›i (HybridSNAP + Explain + Cost-Performance + Guard).

from __future__ import annotations

import argparse
import sys
import time
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

# Cho phÃ©p import src.* dÃ¹ script náº±m ngoÃ i thÆ° má»¥c src
ROOT_DIR = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT_DIR))

# Kiá»ƒm tra sys.path
print("Current sys.path:", sys.path)

# Utils config
from src.utils.config import load_config

# Loader chuáº©n (S-NAP, A-SAD, T-SAD + split train/val/test)
from src.data.loader import load_task_csv

# Core models
from src.sequence.ensemble import SequenceEnsemble
from src.graph_reasoning.dfg import DFG
from src.graph_reasoning.reasoner import GraphReasoner, GraphReasonerConfig
from src.graph_reasoning.semantics_prior import SemanticsPrior, SemanticsPriorConfig
from src.semantics.prompt_pool import PromptPool, PromptPoolConfig
from src.semantics.inference import SemanticsLLM, RuntimeConfig, GenConfig
from src.hybrid.pipeline import HybridSNAP, HybridConfig
from src.semantics.instruction_memory import InstructionMemory

# Guard
from src.guard.config import GuardConfig
from src.guard.core import Guard

# Eval
from src.eval.runner import run_evaluation, EvalSample

# Kiá»ƒm tra bá»™ nhá»› GPU trÆ°á»›c khi cháº¡y
def check_gpu_memory():
    if torch.cuda.is_available():
        free_mem, total_mem = torch.cuda.mem_get_info(0)
        used_mem = total_mem - free_mem
        print(f"GPU Memory - Free: {free_mem / 1024**3:.2f} GB, Used: {used_mem / 1024**3:.2f} GB, Total: {total_mem / 1024**3:.2f} GB")
        if used_mem / total_mem > 0.8:  # Giá»›i háº¡n sá»­ dá»¥ng bá»™ nhá»› 80%
            print("âš ï¸ Cáº£nh bÃ¡o: Bá»™ nhá»› GPU sá»­ dá»¥ng quÃ¡ cao, cÃ¢n nháº¯c giáº£m batch size hoáº·c sequence length.")
    else:
        print("ğŸ”´ GPU khÃ´ng kháº£ dá»¥ng.")

# =========================
# GPU monitor (tuá»³ chá»n)
# =========================
def _gpu_snapshot() -> str:
    """
    Cá»‘ gáº¯ng láº¥y thÃ´ng tin GPU/VRAM theo thá»© tá»± Æ°u tiÃªn:
    1) pynvml (á»•n Ä‘á»‹nh nháº¥t)
    2) nvidia-smi (subprocess)
    3) torch.cuda.mem_get_info (cuá»‘i cÃ¹ng)
    """
    # 1) pynvml
    try:
        import pynvml  # type: ignore

        pynvml.nvmlInit()
        h = pynvml.nvmlDeviceGetHandleByIndex(0)
        name = pynvml.nvmlDeviceGetName(h).decode("utf-8")
        mem = pynvml.nvmlDeviceGetMemoryInfo(h)
        util = pynvml.nvmlDeviceGetUtilizationRates(h).gpu
        txt = f"{name} | util={util:>3}% | VRAM={mem.used//(1024**2)}/{mem.total//(1024**2)} MiB"
        pynvml.nvmlShutdown()
        return txt
    except Exception:
        pass

    # 2) nvidia-smi
    try:
        import subprocess, shlex

        cmd = "nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits"
        out = subprocess.check_output(shlex.split(cmd), stderr=subprocess.DEVNULL, text=True, timeout=2)
        name, util, used, total = [x.strip() for x in out.split(",")]
        util = util if util != "[Unknown Error]" else "?"
        return f"{name} | util={util}% | VRAM={used}/{total} MiB"
    except Exception:
        pass

    # 3) torch (free/total)
    try:
        import torch  # type: ignore

        if torch.cuda.is_available():
            free, total = torch.cuda.mem_get_info()
            name = torch.cuda.get_device_name(0)
            used = (total - free) // (1024**2)
            total_mb = total // (1024**2)
            return f"{name} | VRAMâ‰ˆ{used}/{total_mb} MiB"
    except Exception:
        pass

    return "GPU monitor unavailable"


def _gpu_monitor_loop(stop_flag, interval_sec: float = 2.0) -> None:
    while not stop_flag.is_set():
        print(f"[GPU] {_gpu_snapshot()}")
        stop_flag.wait(interval_sec)


# =========================
# Print config summary (schema má»›i)
# =========================
def _print_cfg_summary(cfg: Dict[str, Any]) -> None:
    dataset = cfg.get("dataset", {}) or {}
    data = cfg.get("data", {}) or {}
    seq = cfg.get("sequence", {}) or {}
    graph = cfg.get("graph", {}) or {}
    sem = cfg.get("semantics", {}) or {}
    semp = cfg.get("semantics_prior", {}) or {}
    guard = cfg.get("guard", {}) or {}
    hybrid = cfg.get("hybrid", {}) or {}
    evl = cfg.get("eval", {}) or {}
    paths = cfg.get("paths", {}) or {}

    print("âš™ï¸  Cáº¥u hÃ¬nh tÃ³m táº¯t")
    print("   â”œâ”€ run_name    :", cfg.get("run_name"))
    print("   â”œâ”€ task        :", cfg.get("task", "next_activity"))
    print("   â”œâ”€ dataset     :", dataset.get("path"))
    print("   â”œâ”€ split_file  :", dataset.get("split_file"))
    print("   â”œâ”€ case_id_col :", dataset.get("case_id_col", "model_id"))
    print("   â”œâ”€ prefix_col  :", dataset.get("prefix_col", "prefix"))
    print("   â”œâ”€ label_col   :", dataset.get("label_col", "next"))
    print("   â”œâ”€ prefix_sep  :", dataset.get("prefix_sep", ";"))
    print(
        "   â”œâ”€ data        :",
        f"drop_end={data.get('drop_end')}, invert_labels={data.get('invert_labels')}, "
        f"min_activities={data.get('min_activities')}, load_limit={data.get('load_limit')}",
    )
    print("   â”œâ”€ output_dir  :", paths.get("output_dir", "outputs"))
    print(
        "   â”œâ”€ sequence    :",
        f"alpha={seq.get('alpha',0.5)}, "
        f"markov_order={seq.get('markov_order',2)}, min_count={seq.get('min_count',1)}",
    )
    print(
        "   â”œâ”€ graph       :",
        f"enabled={graph.get('enabled',True)}, "
        f"dfg(min_count={graph.get('dfg',{}).get('min_count',1)}, "
        f"min_ratio={graph.get('dfg',{}).get('min_ratio',0.0)}), "
        f"reasoner={graph.get('reasoner',{})}",
    )
    print(
        "   â”œâ”€ semantics   :",
        f"use_llm={sem.get('use_llm',False)}, model={sem.get('model_name')}, "
        f"4bit={sem.get('load_in_4bit',False)}, max_len={sem.get('max_seq_len',2048)}, mode={sem.get('mode','logprob')}",
    )
    print("   â”œâ”€ sem_prior   :", semp)
    print("   â”œâ”€ guard       :", guard)
    print("   â”œâ”€ hybrid      :", hybrid)
    print(
        "   â””â”€ eval        :",
        f"num_samples={evl.get('num_samples')}, ndcg_k={evl.get('ndcg_k',5)}, "
        f"measure_cost={evl.get('measure_cost',False)}",
    )
    print()


# =========================
# Main
# =========================
def main() -> None:
    # Khá»Ÿi táº¡o parser cho cÃ¡c tham sá»‘ dÃ²ng lá»‡nh
    parser = argparse.ArgumentParser(description="Run Local Evaluation for S-NAPX Hybrid")

    # Tham sá»‘ cho file cáº¥u hÃ¬nh (cÃ³ thá»ƒ truyá»n nhiá»u file YAML)
    parser.add_argument(
        "--config",
        "-c",
        type=str,
        nargs="+",
        required=True,
        help="ÄÆ°á»ng dáº«n tá»›i 1 hoáº·c nhiá»u file YAML cáº¥u hÃ¬nh (vd: config/base.yaml config/local_eval.yaml)",
    )

    # Tham sá»‘ Ä‘á»ƒ báº­t cháº¿ Ä‘á»™ theo dÃµi GPU/VRAM
    parser.add_argument(
        "--monitor-gpu",
        action="store_true",
        help="In tráº¡ng thÃ¡i GPU/VRAM má»—i vÃ i giÃ¢y trong khi cháº¡y",
    )

    # Tham sá»‘ chu ká»³ in tráº¡ng thÃ¡i GPU
    parser.add_argument(
        "--gpu-interval",
        type=float,
        default=2.0,
        help="Chu ká»³ (giÃ¢y) in tráº¡ng thÃ¡i GPU khi báº­t --monitor-gpu",
    )

    # Tham sá»‘ Ä‘á»ƒ lÆ°u cáº¥u hÃ¬nh Ä‘Ã£ gá»™p vÃ o thÆ° má»¥c output
    parser.add_argument(
        "--save-merged-cfg",
        action="store_true",
        help="LÆ°u báº£n config Ä‘Ã£ merge vÃ o thÆ° má»¥c output Ä‘á»ƒ tÃ¡i láº­p thÃ­ nghiá»‡m",
    )

    # Tham sá»‘ Ä‘Æ°á»ng dáº«n tá»›i file phÃ¢n chia train/val/test
    parser.add_argument(
        "--split-file",
        type=str,
        required=True,
        help="ÄÆ°á»ng dáº«n tá»›i file phÃ¢n chia train/val/test (vÃ­ dá»¥: datasets/snap_train_val_test.pkl)",
    )

    # PhÃ¢n tÃ­ch cÃ¡c tham sá»‘ dÃ²ng lá»‡nh
    args = parser.parse_args()

    # Äá»c cÃ¡c file cáº¥u hÃ¬nh tá»« tham sá»‘ --config
    cfg_paths = [str(Path(p)) for p in args.config]

    # 1) Load & merge cÃ¡c file YAML cáº¥u hÃ¬nh
    cfg = load_config(cfg_paths)

    # Ghi Ä‘Ã¨ dataset.split_file báº±ng CLI --split-file Ä‘á»ƒ Ä‘áº£m báº£o dÃ¹ng Ä‘Ãºng file pkl
    if "dataset" not in cfg or cfg["dataset"] is None:
        cfg["dataset"] = {}
    cfg["dataset"]["split_file"] = args.split_file

    # 2) Chuáº©n bá»‹ thÆ° má»¥c output
    paths_cfg = cfg.get("paths", {})
    run_name = cfg.get("run_name", "run_local")  # TÃªn cháº¡y, máº·c Ä‘á»‹nh lÃ  "run_local"
    base_output = Path(paths_cfg.get("output_dir", "outputs"))  # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c output
    out_dir = base_output / run_name  # Táº¡o thÆ° má»¥c cho má»—i láº§n cháº¡y
    out_dir.mkdir(parents=True, exist_ok=True)  # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³

    # In thÃ´ng bÃ¡o báº¯t Ä‘áº§u cháº¡y mÃ´ hÃ¬nh
    print("\nğŸš€  Báº¯t Ä‘áº§u cháº¡y S-NAPX Hybrid (Local Mode)...\n")

    # In ra cáº¥u hÃ¬nh tÃ³m táº¯t (Ä‘á»ƒ kiá»ƒm tra trÆ°á»›c khi cháº¡y)
    _print_cfg_summary(cfg)

    # LÆ°u báº£n cáº¥u hÃ¬nh Ä‘Ã£ gá»™p náº¿u tham sá»‘ --save-merged-cfg Ä‘Æ°á»£c báº­t
    if args.save_merged_cfg:
        merged_cfg_path = out_dir / "run_config_merged.yaml"
        try:
            with open(merged_cfg_path, "w", encoding="utf-8") as f:
                yaml.safe_dump(cfg, f, allow_unicode=True)
            print(f"ğŸ“  ÄÃ£ lÆ°u merged config â†’ {merged_cfg_path}")
        except Exception as e:
            print(f"[Warn] KhÃ´ng thá»ƒ lÆ°u merged config: {e}")

    # GPU monitor (tuá»³ chá»n)
    stop_flag = threading.Event()
    t_monitor = None
    if args.monitor_gpu:
        t_monitor = threading.Thread(
            target=_gpu_monitor_loop,
            args=(stop_flag, args.gpu_interval),
            daemon=True,
        )
        t_monitor.start()

    t0 = time.time()
    try:
        # ==============================
        # 3) Load dá»¯ liá»‡u báº±ng load_task_csv (S-NAP + split train/test)
        # ==============================
        dataset_cfg = cfg.get("dataset", {}) or {}
        data_cfg = cfg.get("data", {}) or {}
        eval_cfg = cfg.get("eval", {}) or {}
        task = cfg.get("task", "next_activity")

        dataset_path = dataset_cfg.get("path")
        split_file = dataset_cfg.get("split_file")
        if not dataset_path:
            raise ValueError("dataset.path chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong YAML.")
        if not split_file:
            raise ValueError("dataset.split_file hoáº·c --split-file chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p.")

        load_limit = data_cfg.get("load_limit", None)
        drop_end = data_cfg.get("drop_end", True)
        invert_labels = data_cfg.get("invert_labels", False)
        min_acts = data_cfg.get("min_activities", 2)

        ret = load_task_csv(
            task=task,
            dataset_path=dataset_path,
            limit=load_limit,
            drop_end=drop_end,
            invert_labels=invert_labels,
            min_activities=min_acts,
            split_file=split_file,
            return_splits_if_available=True,
        )

        # Náº¿u cÃ³ split_file há»£p lá»‡: ret lÃ  dict {'train','val','test'}
        if isinstance(ret, dict):
            df_train = ret["train"]
            df_test = ret["test"]
            df_val = ret.get("val")
        else:
            df_train = df_test = ret
            df_val = None

        print(f"ğŸ“¦  Loaded train={len(df_train)}, test={len(df_test)} tá»« {dataset_path}")

        # Eval trÃªn test (cÃ³ thá»ƒ giá»›i háº¡n báº±ng eval.num_samples)
        num_samples = eval_cfg.get("num_samples")
        if isinstance(num_samples, int) and 0 < num_samples < len(df_test):
            df_eval = df_test.head(num_samples)
            print(f"ğŸ”  Giá»›i háº¡n xuá»‘ng {len(df_eval)} máº«u test Ä‘á»ƒ eval nhanh.")
        else:
            df_eval = df_test

        # Táº¡o EvalSample tá»« df_eval (cÃ³ ná»‘i gold_explanation náº¿u cÃ³ cá»™t 'output')
        samples: List[EvalSample] = []
        has_output_col = "output" in df_eval.columns

        for _, row in df_eval.iterrows():
            pref = list(row["prefix"])
            y_true = str(row["next"])
            mid = str(row.get("model_id", ""))
            rid = str(row.get("revision_id", "")) if "revision_id" in row else ""
            case_id = f"{mid}_{rid}" if rid != "" else mid

            gold_expl: Optional[str] = None
            if has_output_col:
                v = str(row.get("output", "")).strip()
                if v and v.lower() != "nan":
                    gold_expl = v

            samples.append(
                EvalSample(
                    case_id=case_id,
                    prefix=pref,
                    y_true=y_true,
                    gold_explanation=gold_expl,
                )
            )

        if not samples:
            raise ValueError("KhÃ´ng cÃ³ sample nÃ o sau khi táº¡o EvalSample.")

        print(f"ğŸ”  Eval samples: {len(samples)}")

        # Traces cho Sequence & DFG láº¥y tá»« TRAIN (Ä‘Ãºng methodology)
        traces_for_seq: List[List[str]] = []
        for _, row in df_train.iterrows():
            pref = list(row["prefix"])
            nxt = str(row["next"])
            if pref:
                traces_for_seq.append(pref + [nxt])
            else:
                traces_for_seq.append([nxt])

        activities_vocab = sorted({act for tr in traces_for_seq for act in tr})
        print(f"ğŸ§©  Num activities in vocab: {len(activities_vocab)}")

        # =======================================
        # 4) SequenceEnsemble + DFG + Reasoner
        # =======================================
        seq_cfg = cfg.get("sequence", {}) or {}
        seq_model = SequenceEnsemble(
            alpha=seq_cfg.get("alpha", 0.5),
            markov_order=seq_cfg.get("markov_order", 2),
            min_count=seq_cfg.get("min_count", 1),
        )
        seq_model.fit(traces_for_seq)
        print("âœ…  SequenceEnsemble fitted.")

        graph_cfg = cfg.get("graph", {}) or {}
        dfg_cfg = graph_cfg.get("dfg", {}) or {}
        dfg = DFG.from_traces(
            traces_for_seq,
            min_count=dfg_cfg.get("min_count", 1),
            min_ratio=dfg_cfg.get("min_ratio", 0.0),
        )
        print("âœ…  DFG built.")

        reasoner_cfg = graph_cfg.get("reasoner", {}) or {}
        graph_reasoner = GraphReasoner(
            dfg=dfg,
            cfg=GraphReasonerConfig(
                enabled=graph_cfg.get("enabled", True),
                hard_mask=reasoner_cfg.get("hard_mask", False),
                boost_factor=reasoner_cfg.get("boost_factor", 0.5),
                min_prob_keep=reasoner_cfg.get("min_prob_keep", 0.0),
            ),
        )

        # =======================
        # 5) Semantics-Prior
        # =======================
        import json

        semp_cfg = cfg.get("semantics_prior", {}) or {}
        sem_prior_conf = SemanticsPriorConfig(
            enabled=semp_cfg.get("enabled", False),
            mode=semp_cfg.get("mode", "activity"),
            default_weight=semp_cfg.get("default_weight", 1.0),
            lambda_mix=semp_cfg.get("lambda_mix", 0.2),
            floor=semp_cfg.get("floor", 0.7),
            ceil=semp_cfg.get("ceil", 1.3),
        )
        prior_map = {}
        prior_path = semp_cfg.get("prior_map_path")
        if prior_path:
            try:
                with open(prior_path, "r", encoding="utf-8") as f:
                    prior_map = json.load(f)
                print(f"âœ…  Loaded semantics prior map tá»« {prior_path}")
            except Exception as e:
                print(f"[Warn] KhÃ´ng Ä‘á»c Ä‘Æ°á»£c prior_map tá»« {prior_path}: {e}")
        sem_prior = SemanticsPrior(cfg=sem_prior_conf, prior_map=prior_map)

        # =======================
        # 6) PromptPool + LLM + IT few-shot
        # =======================
        prompt_pool = PromptPool(PromptPoolConfig())
        sem_cfg = cfg.get("semantics", {}) or {}
        use_llm = sem_cfg.get("use_llm", False)
        sem_llm: Optional[SemanticsLLM] = None

        # IT few-shot memory (InstructionMemory)
        instruction_memory = None
        it_cfg = sem_cfg.get("it_memory", {}) or {}
        it_enabled = it_cfg.get("enabled", False)

        if use_llm and it_enabled:
            # Æ°u tiÃªn path trong it_memory; náº¿u khÃ´ng cÃ³ thÃ¬ fallback dataset.path
            dataset_cfg_for_it = cfg.get("dataset", {}) or {}
            it_dataset_path = it_cfg.get("dataset_path") or dataset_cfg_for_it.get("path")

            if it_dataset_path:
                try:
                    instruction_memory = InstructionMemory.from_csv(
                        dataset_path=it_dataset_path,
                        limit=it_cfg.get("limit"),
                        max_examples=it_cfg.get("max_examples"),
                        min_prefix_len=it_cfg.get("min_prefix_len", 1),
                    )
                    print(
                        f"âœ…  InstructionMemory loaded from {it_dataset_path} "
                        f"with {len(instruction_memory.examples)} examples."
                    )
                except Exception as e:
                    print(f"[Warn] KhÃ´ng load Ä‘Æ°á»£c InstructionMemory tá»« {it_dataset_path}: {e}")
                    instruction_memory = None
            else:
                print(
                    "[Warn] semantics.it_memory.enabled=True nhÆ°ng khÃ´ng tÃ¬m Ä‘Æ°á»£c dataset_path "
                    "(thiáº¿u semantics.it_memory.dataset_path hoáº·c dataset.path)."
                )

        if use_llm:
            print("ğŸ§   Khá»Ÿi táº¡o SemanticsLLM (cÃ³ thá»ƒ tá»‘n thá»i gian / VRAM)...")
            runtime_cfg = RuntimeConfig(
                model_name=sem_cfg.get("model_name", "microsoft/Phi-3-mini-4k-instruct"),
                max_seq_len=sem_cfg.get("max_seq_len", 2048),
                load_in_4bit=sem_cfg.get("load_in_4bit", False),
                device=sem_cfg.get("device", "auto"),
            )
            gen_cfg = GenConfig(
                max_new_tokens=sem_cfg.get("gen", {}).get("max_new_tokens", 16),
                temperature=sem_cfg.get("gen", {}).get("temperature", 0.0),
                top_p=sem_cfg.get("gen", {}).get("top_p", 1.0),
                do_sample=sem_cfg.get("gen", {}).get("do_sample", False),
            )
            mode = sem_cfg.get("mode", "logprob")
            use_it_fewshot = sem_cfg.get("use_it_fewshot", False)
            it_num_shots = sem_cfg.get("it_num_shots", 3)

            try:
                sem_llm = SemanticsLLM(
                    prompt_pool=prompt_pool,
                    runtime_cfg=runtime_cfg,
                    gen_cfg=gen_cfg,
                    mode=mode,
                    instruction_memory=instruction_memory,
                    use_it_fewshot=use_it_fewshot,
                    it_num_shots=it_num_shots,
                )
                print("âœ…  SemanticsLLM ready trÃªn device:", sem_llm.device)
            except Exception as e:
                print(f"[Warn] KhÃ´ng khá»Ÿi táº¡o Ä‘Æ°á»£c SemanticsLLM, fallback sequence-only: {e}")
                sem_llm = None
                use_llm = False
        else:
            print("â„¹ï¸  semantics.use_llm = False â†’ cháº¡y sequence + graph (khÃ´ng dÃ¹ng LLM).")

        # =======================
        # 6b) Guard (footprint / anomaly) â€“ FAIL-SOFT
        # =======================
        guard_cfg_raw = cfg.get("guard", {}) or {}
        guard = None
        guard_enabled_flag = guard_cfg_raw.get("enabled", False)

        if guard_enabled_flag:
            gcfg = GuardConfig(
                enabled=True,
                mode=guard_cfg_raw.get("mode", "soft"),
                use_activity_guard=guard_cfg_raw.get("use_activity_guard", True),
                use_trace_guard=guard_cfg_raw.get("use_trace_guard", False),
                penalty_factor=guard_cfg_raw.get("penalty_factor", 0.5),
                min_prefix_len=guard_cfg_raw.get("min_prefix_len", 1),
                activity_dataset=guard_cfg_raw.get("activity_dataset"),
                trace_dataset=guard_cfg_raw.get("trace_dataset"),
            )
            try:
                guard = Guard.from_datasets(gcfg)
                print("âœ…  Guard loaded tá»« datasets (activity/trace).")
            except Exception as e:
                # KhÃ´ng Ä‘á»ƒ Guard lÃ m crash toÃ n bá»™ pipeline
                print(f"[Warn] KhÃ´ng build Ä‘Æ°á»£c Guard tá»« datasets, táº¡m thá»i táº¯t Guard: {e}")
                guard = None
                guard_enabled_flag = False
        else:
            print("â„¹ï¸  guard.enabled = False â†’ khÃ´ng dÃ¹ng Guard.")

        # =======================
        # 7) HybridSNAP
        # =======================
        hybrid_cfg_raw = cfg.get("hybrid", {}) or {}
        hyb_cfg = HybridConfig(
            top_k=hybrid_cfg_raw.get("top_k", 5),
            enable_graph_reasoner=hybrid_cfg_raw.get("enable_graph_reasoner", True),
            enable_semantics_prior=hybrid_cfg_raw.get(
                "enable_semantics_prior",
                semp_cfg.get("enabled", False),
            ),
            # náº¿u YAML khÃ´ng ghi rÃµ, máº·c Ä‘á»‹nh bÃ¡m theo guard_enabled_flag
            enable_guard=hybrid_cfg_raw.get("enable_guard", guard_enabled_flag),
            enable_explanation=hybrid_cfg_raw.get("enable_explanation", False) and bool(sem_llm),
        )

        hybrid = HybridSNAP(
            cfg=hyb_cfg,
            seq_model=seq_model,
            graph_reasoner=graph_reasoner if hyb_cfg.enable_graph_reasoner else None,
            sem_prior=sem_prior if hyb_cfg.enable_semantics_prior else None,
            semantics_llm=sem_llm,
            guard=guard,
            prompt_pool=prompt_pool,
            activities_vocab=activities_vocab,
        )
        print("âœ…  HybridSNAP ready.")

        # =======================
        # 8) Evaluation
        # =======================
        ndcg_k = eval_cfg.get("ndcg_k", 5)
        measure_cost = eval_cfg.get("measure_cost", False)

        eval_result = run_evaluation(
            samples=samples,
            model=hybrid,
            activities_vocab=activities_vocab,
            measure_cost=measure_cost,
            ndcg_k=ndcg_k,
        )

    finally:
        if args.monitor_gpu:
            stop_flag.set()
            if t_monitor is not None:
                t_monitor.join(timeout=1)

    elapsed = time.time() - t0

    # =======================
    # 9) In & lÆ°u káº¿t quáº£
    # =======================
    print("\nğŸ“Š  Káº¿t quáº£ tá»•ng há»£p:")
    print("----------------------")
    print(f"Accuracy     : {eval_result.accuracy:.4f}")
    print(f"Macro-F1     : {eval_result.macro_f1:.4f}")
    print(f"MRR          : {eval_result.mrr:.4f}")
    print(f"NDCG@{ndcg_k:<3}: {eval_result.ndcg_at_k:.4f}")

    print("\nğŸ”  Explain Metrics:")
    for k, v in eval_result.explain_metrics.items():
        print(f"{k:<25}: {v:.4f}")

    if eval_result.latency is not None:
        lat = eval_result.latency
        print("\nâ±ï¸  Latency (ms):")
        print(f"avg_ms        : {lat.avg_ms:.2f}")
        print(f"p50_ms        : {lat.p50_ms:.2f}")
        print(f"p90_ms        : {lat.p90_ms:.2f}")
        print(f"p95_ms        : {lat.p95_ms:.2f}")

    if eval_result.cpr is not None:
        print(f"\nâš–ï¸  CPR (Macro-F1 / avg_sec): {eval_result.cpr:.4f}")

    print("----------------------")
    print(f"â±ï¸  Thá»i gian cháº¡y: {elapsed/60:.2f} phÃºt ({elapsed:.1f} giÃ¢y)")
    print(f"ğŸ“‚  Output dir   : {out_dir}\n")

    # LÆ°u results.yaml Ä‘á»ƒ sau nÃ y dÃ¹ng cho bÃ¡o cÃ¡o
    results_dict: Dict[str, Any] = {
        "accuracy": eval_result.accuracy,
        "macro_f1": eval_result.macro_f1,
        "mrr": eval_result.mrr,
        "ndcg_at_k": eval_result.ndcg_at_k,
        "explain_metrics": eval_result.explain_metrics,
        "latency_ms": None,
        "cpr": eval_result.cpr,
    }
    if eval_result.latency is not None:
        results_dict["latency_ms"] = {
            "avg_ms": eval_result.latency.avg_ms,
            "p50_ms": eval_result.latency.p50_ms,
            "p90_ms": eval_result.latency.p90_ms,
            "p95_ms": eval_result.latency.p95_ms,
        }

    results_path = out_dir / "results.yaml"
    try:
        with open(results_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(results_dict, f, allow_unicode=True)
        print(f"ğŸ“  ÄÃ£ lÆ°u káº¿t quáº£ chi tiáº¿t â†’ {results_path}")
    except Exception as e:
        print(f"[Warn] KhÃ´ng thá»ƒ lÆ°u results.yaml: {e}")


if __name__ == "__main__":
    main()
